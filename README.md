# SD-VSum Paper Materials

This repository contains the official implementation of the model proposed in the SD-VSum: A Method and Dataset for Script-Driven Video Summarization paper, along with the S-VideoXum dataset. In addition to the full training and evaluation code, it provides all necessary files and annotations required to reproduce the results.

[Paper in arxiv](https://arxiv.org/abs/2505.03319) | [S-VideoXum Dataset in Zenodo](https://zenodo.org/records/15349075)

Section A describes the dataset structure and contents, while Section B details the model implementation and usage.

## A. S-VideoXum dataset

S-VideoXum is a dataset for Script-driven Video Summarization, which derived from the existing VideoXum large-scale dataset (the latter is used for cross-modal video summarization).

You can download the S-VideoXum dataset, including the h5 file with CLIP features, grountruth scores, the json file with the dataset splits, and the text annotations from the following link:

[Zenodo: S-VideoXum: A Dataset for Script-driven Video Summarization](https://zenodo.org/records/15349075)

The original VideoXum dataset (which can be found [here](https://github.com/jylins/videoxum)) includes 14,000 open-domain videos up to 12.5 min. long with diverse visual content, from the ActivityNet Captions dataset (available [here](https://cs.stanford.edu/people/ranjaykrishna/densevid/)). Each video is associated with multiple ground-truth video summaries - in the form of frame-level binary scores which denote the inclusion (label "1") or not (label "0") of a frame in the video summary - that were obtained by 10 different human annotators. The existence of multiple ground-truth summaries per video (10 in total) makes the VideoXum dataset well-suited for extending it to the script-driven video summarization task, as it allows to train a summarization method to generate different summaries for a given video driven by a different script about the content of each individual summary. 

To make VideoXum suitable for training and evaluation of script-driven video summarization methods, we extended it by producing natural language descriptions of the different ground-truth summaries that are available per video. These serve as the scripts that can drive the summarization process. For this, we employed the publicly-available state-of-the-art Large Multimodal Model LLaVA-NeXT-7B. Since some of the videos from the VideoXum dataset were not publicly-available during this work, the extended S-VideoXum dataset for script-driven video summarization differs from the original one in terms of number of videos, including data for 11,908 videos. These we split in 6,782 samples for training, 3,419 for validation and 1,707 for testing.

For experimental comparisons with multimodal approaches for generic video summarization, we produced natural language descriptions of the full-length videos, as well, using the LlaVA-NeXT-7B model and the same prompt. 

The generated natural language descriptions for the ground-truth summaries, i.e., the "scripts" (as well as the natural language descriptions for the full-length videos) of the S-VideoXum dataset, along with the extracted (CLIP-based) embeddings from visual and textual data and the used data splits in our experiments, are publicly-available in the Zenodo repository.

The CLIP embeddings (visual and textual) released with S-VideoXum were extracted using the finetuned CLIP version authors of the original VideoXum released (available [here](https://huggingface.co/jylins/vtsum_blip/blob/main/vt_clip.pth)). For visual embeddings, videos were downsampled at 1 frame per second, while for textual embeddings, scripts and dense captions were split into sentences.

### Folder Structure of the Dataset

dataset/
├── script_videoxum.h5
├── script_videoxum_splits.json
└── Text Annotations/
├── Scripts/
└── Dense Captions/

---
### 1. `script_videoxum.h5`
The core HDF5 file for the proposed dataset. Each top‐level group corresponds to one video, named by its `video_name`. Inside each video group:
- **`gtsummaries`**
  Ground‐truth summaries from the 10 human annotators.  

  Shape: `[10, n_frames]`.


- **`n_frames`**  

  Number of downsampled frames in the video.  

  Type: scalar integer.


- **`video_embeddings`**  

  CLIP feature matrix for each frame.  

  Shape: `[n_frames, 512]`



- **`text_embeddings`**  

  CLIP feature tensor for the 10 scripts (one per annotator).  

  Shape: `[10, M_max, 512]`  

  - `10`: number of annotators  

  - `M_max`: maximum number of sentences across all scripts  

  - Scripts shorter than `M_max` are zero‐padded.



- **`multimodal_text_embeddings`**  

  CLIP feature matrix for the dense video captions (generated by LlaVA‑NeXT).  

  Shape: `[M, 512]`  

  - `M`: number of sentences in the dense captions  

  - Used for multimodal summarization experiments.



---



### 2. `script_videoxum_splits.json`



A JSON file defining train/validation/test splits.



---



### 3. Text Annotations



This folder holds all text files used in the dataset, generated by the Llava‑Next model:



#### 3.1 `Scripts/`



Natural‐language “scripts” extracted from each annotator’s ground‐truth summary.  

Filenames follow the pattern:

video_name_annotator_id.txt # annotator_id ∈ [0, 9]



#### 3.2 `Dense Captions/`



Dense video captions automatically generated for each video.  

Filenames follow the pattern:

video_name.txt


## B. SD-VSum method and models

These materials will be released at a later date.

## Citation

The S-VideoXum dataset is proposed our paper: 
M. Mylonas, E. Apostolidis, V. Mezaris, "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", arXiv:2505.03319, [doi: 10.48550/arXiv.2505.03319](https://doi.org/10.48550/arXiv.2505.03319). 

If you find this dataset interesting or useful in your research, use the following Bibtex annotation to cite us:

```bibtex
@misc{mylonas2025sdvsummethoddatasetscriptdriven,
      title={SD-VSum: A Method and Dataset for Script-Driven Video Summarization}, 
      author={Manolis Mylonas and Evlampios Apostolidis and Vasileios Mezaris},
      year={2025},
      eprint={2505.03319},
      url={https://arxiv.org/abs/2505.03319} 
}
```

## License
This code is provided for academic, non-commercial use only. Please also check for any restrictions applied in the code parts and datasets used here from other sources. For the materials not covered by any such restrictions, redistribution and use in source and binary forms, with or without modification, are permitted for academic non-commercial use provided that the following conditions are met:

Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation provided with the distribution. 

This software is provided by the authors "as is" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the authors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.
