# SD-VSum Paper Materials

This repository contains the official implementation of the model proposed in the SD-VSum: A Method and Dataset for Script-Driven Video Summarization paper, along with the S-VideoXum dataset. In addition to the full training and evaluation code, it provides all necessary files and annotations required to reproduce the results.

[Paper in arxiv](https://arxiv.org/abs/2505.03319) | [S-VideoXum Dataset in Zenodo](https://zenodo.org/records/15349075)

Section A describes the dataset structure and contents, while Section B details the model implementation and usage.

## A. S-VideoXum dataset

S-VideoXum is a dataset for Script-driven Video Summarization, which derived from the existing VideoXum large-scale dataset (the latter is used for cross-modal video summarization).

You can download the S-VideoXum dataset, including the h5 file with CLIP features, grountruth scores, the json file with the dataset splits, and the text annotations from the following link:

[Zenodo: S-VideoXum: A Dataset for Script-driven Video Summarization](https://zenodo.org/records/15349075)

The original VideoXum dataset (which can be found [here](https://github.com/jylins/videoxum)) includes 14,000 open-domain videos up to 12.5 min. long with diverse visual content, from the ActivityNet Captions dataset (available [here](https://cs.stanford.edu/people/ranjaykrishna/densevid/)). Each video is associated with multiple ground-truth video summaries - in the form of frame-level binary scores which denote the inclusion (label "1") or not (label "0") of a frame in the video summary - that were obtained by 10 different human annotators. The existence of multiple ground-truth summaries per video (10 in total) makes the VideoXum dataset well-suited for extending it to the script-driven video summarization task, as it allows to train a summarization method to generate different summaries for a given video driven by a different script about the content of each individual summary. 

To make VideoXum suitable for training and evaluation of script-driven video summarization methods, we extended it by producing natural language descriptions of the different ground-truth summaries that are available per video. These serve as the scripts that can drive the summarization process. For this, we employed the publicly-available state-of-the-art Large Multimodal Model LLaVA-NeXT-7B. Since some of the videos from the VideoXum dataset were not publicly-available during this work, the extended S-VideoXum dataset for script-driven video summarization differs from the original one in terms of number of videos, including data for 11,908 videos. These we split in 6,782 samples for training, 3,419 for validation and 1,707 for testing.

For experimental comparisons with multimodal approaches for generic video summarization, we produced natural language descriptions of the full-length videos, as well, using the LlaVA-NeXT-7B model and the same prompt. 

The generated natural language descriptions for the ground-truth summaries, i.e., the "scripts" (as well as the natural language descriptions for the full-length videos) of the S-VideoXum dataset, along with the extracted (CLIP-based) embeddings from visual and textual data and the used data splits in our experiments, are publicly-available in the Zenodo repository.

The CLIP embeddings (visual and textual) released with S-VideoXum were extracted using the finetuned CLIP version authors of the original VideoXum released (available [here](https://huggingface.co/jylins/vtsum_blip/blob/main/vt_clip.pth)). For visual embeddings, videos were downsampled at 1 frame per second, while for textual embeddings, scripts and dense captions were split into sentences.

### Folder Structure of the Dataset

```
dataset/
├── script_videoxum.h5
├── script_videoxum_splits.json
└── Text Annotations/
      ├── Scripts/
      └── Dense Captions/
```

---
### 1. `script_videoxum.h5`
The core HDF5 file for the proposed dataset. Each top‐level group corresponds to one video, named by its `video_name`. Inside each video group:
- **`gtsummaries`**
  Ground‐truth summaries from the 10 human annotators.  

  Shape: `[10, n_frames]`.


- **`n_frames`**  

  Number of downsampled frames in the video.  

  Type: scalar integer.


- **`video_embeddings`**  

  CLIP feature matrix for each frame.  

  Shape: `[n_frames, 512]`



- **`text_embeddings`**  

  CLIP feature tensor for the 10 scripts (one per annotator).  

  Shape: `[10, M_max, 512]`  

  - `10`: number of annotators  

  - `M_max`: maximum number of sentences across all scripts  

  - Scripts shorter than `M_max` are zero‐padded.



- **`multimodal_text_embeddings`**  

  CLIP feature matrix for the dense video captions (generated by LlaVA‑NeXT).  

  Shape: `[M, 512]`  

  - `M`: number of sentences in the dense captions  

  - Used for multimodal summarization experiments.



---



### 2. `script_videoxum_splits.json`



A JSON file defining train/validation/test splits.



---



### 3. Text Annotations



This folder holds all text files used in the dataset, generated by the Llava‑Next model:



#### 3.1 `Scripts/`



Natural‐language “scripts” extracted from each annotator’s ground‐truth summary.  

Filenames follow the pattern:

video_name_annotator_id.txt # annotator_id ∈ [0, 9]



#### 3.2 `Dense Captions/`



Dense video captions automatically generated for each video.  

Filenames follow the pattern:

video_name.txt


## B. SD-VSum method and models

This repository implements SD-VSum, a method for script-driven video summarization, and provides scripts to train, test, and run the model in inference.

### Installation

Clone the repository
   ```
   git clone https://github.com/IDT-ITI/SD-VSum.git
   cd SD-VSum
   ```
Create and activate the Conda environment
   ```
   conda env create -f environment.yml
   conda activate sd_vsum
   ```
### Dataset Preparation

Download the S-VideoXum dataset
   - Retrieve the .h5 file and the splits.json file from [Zenodo link](https://zenodo.org/records/15349075)
   - Place both files under the ```dataset``` directory
      ```
      SD-VSum
       └── dataset/
            ├── script_videoxum.h5
            ├── script_videoxum_splits.h5
            ├── S_NewsVSum.h5
            └── S_NewsVSum_splits.json
      ```

### Pretrained Models

Download the pretrained SD-VSum model (trained on S-VideoXum) from the Zenodo directory and place it in a local folder.

### Usage

#### Inference on Pretrained Model
To run inference on S-VideoXum using the pretrained checkpoint:
```
python main.py --exp_num='exp1' --ckpt_path='path/to/pkl/file' --train=False --dataset='S_VideoXum'
```

#### Train model on S-VideoXum
S-VideoXum has a {train, validation, test} split. After each training epoch, the model is evaluated on the validation set. After the training is completed, the model that performed better on the validation set is evaluated on the test set. The checkpoint of this model is saved as a .pkl file. To train a model on the S-VideoXum dataset, use the following command:
```
python main.py --exp_num='exp2' --epochs=E --batch_size=B --train=True --dataset='S_VideoXum'
```

#### Train model on S-NewsVSum
S-NewsVSum has a 5-fold split. To train the SD-VSum model in this dataset, use the following commands:
```
python main.py --exp_num='exp3' --epochs=E --batch_size=B --train=True --dataset='S_NewsVSum' --split_num=0
python main.py --exp_num='exp4' --epochs=E --batch_size=B --train=True --dataset='S_NewsVSum' --split_num=1
python main.py --exp_num='exp5' --epochs=E --batch_size=B --train=True --dataset='S_NewsVSum' --split_num=2
python main.py --exp_num='exp6' --epochs=E --batch_size=B --train=True --dataset='S_NewsVSum' --split_num=3
python main.py --exp_num='exp7' --epochs=E --batch_size=B --train=True --dataset='S_NewsVSum' --split_num=4
```

## Citation

The S-VideoXum dataset is proposed in our paper: 
M. Mylonas, E. Apostolidis, V. Mezaris, "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", arXiv:2505.03319, [doi: 10.48550/arXiv.2505.03319](https://doi.org/10.48550/arXiv.2505.03319). 

If you find this dataset interesting or useful in your research, use the following Bibtex annotation to cite us:

```bibtex
@misc{mylonas2025sdvsummethoddatasetscriptdriven,
      title={SD-VSum: A Method and Dataset for Script-Driven Video Summarization}, 
      author={Manolis Mylonas and Evlampios Apostolidis and Vasileios Mezaris},
      year={2025},
      eprint={2505.03319},
      url={https://arxiv.org/abs/2505.03319} 
}
```


