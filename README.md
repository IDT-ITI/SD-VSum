# SD-VSum

This is S-VideoXum, a dataset for Script-driven Video Summarization, derived from the existing VideoXum large-scale dataset (the latter is used for cross-modal video summarization).

You can download the S-VideoXum dataset, including the h5 file with CLIP features, grountruth scores, the json file with the dataset splits, and the text annotations from the following link:

[Zenodo: S-VideoXum: A Dataset for Script-driven Video Summarization](https://zenodo.org/records/15349075)

The original VideoXum dataset includes 14,000 open-domain videos up to 12.5 min. long with diverse visual content, from the ActivityNet Captions dataset. Each video is associated with multiple ground-truth video summaries - in the form of frame-level binary scores which denote the inclusion (label "1") or not (label "0") of a frame in the video summary - that were obtained by 10 different human annotators. The existence of multiple ground-truth summaries per video (10 in total) makes the VideoXum dataset well-suited for extending it to the script-driven video summarization task, as it allows to train a summarization method to generate different summaries for a given video driven by a different script about the content of each individual summary. 

To make VideoXum suitable for training and evaluation of script-driven video summarization methods, we extended it by producing natural language descriptions of the different ground-truth summaries that are available per video. These serve as the scripts that can drive the summarization process. For this, we employed the publicly-available state-of-the-art Large Multimodal Model LLaVA-NeXT-7B. Since some of the videos from the VideoXum dataset were not publicly-available during this work, the extended S-VideoXum dataset for script-driven video summarization differs from the original one in terms of number of videos, including data for 11,908 videos. These we split in 6,782 samples for training, 3,419 for validation and 1,707 for testing.

The generated natural language descriptions for the ground-truth summaries, i.e., the "scripts" (as well as similarly-derived natural language descriptions for the full-length videos) of the S-VideoXum dataset, along with the extracted (CLIP-based) embeddings from visual and textual data and the used data splits in our experiments, are publicly-available in the present repository.

The code for the SD-VSum model is coming soon!

### Folder Structure of the Dataset

dataset/
├── script_videoxum.h5
├── script_videoxum_splits.json
└── Text Annotations/
├── Scripts/
└── Dense Captions/

---
## 1. `script_videoxum.h5`
The core HDF5 file for the proposed dataset. Each top‐level group corresponds to one video, named by its `video_name`. Inside each video group:
- **`gtsummaries`**
  Ground‐truth summaries from the 10 human annotators.  

  Shape: `[10, n_frames]`.


- **`n_frames`**  

  Number of downsampled frames in the video.  

  Type: scalar integer.


- **`video_embeddings`**  

  CLIP feature matrix for each frame.  

  Shape: `[n_frames, 512]`



- **`text_embeddings`**  

  CLIP feature tensor for the 10 scripts (one per annotator).  

  Shape: `[10, M_max, 512]`  

  - `10`: number of annotators  

  - `M_max`: maximum number of sentences across all scripts  

  - Scripts shorter than `M_max` are zero‐padded.



- **`multimodal_text_embeddings`**  

  CLIP feature matrix for the dense video captions (generated by Llava‑Next).  

  Shape: `[M, 512]`  

  - `M`: number of sentences in the dense captions  

  - Used for multimodal summarization experiments.



---



## 2. `script_videoxum_splits.json`



A JSON file defining train/validation/test splits.



---



## 3. Text Annotations



This folder holds all text files used in the dataset, generated by the Llava‑Next model:



### 3.1 `Scripts/`



Natural‐language “scripts” extracted from each annotator’s ground‐truth summary.  

Filenames follow the pattern:

video_name_annotator_id.txt # annotator_id ∈ [0, 9]



### 3.2 `Dense Captions/`



Dense video captions automatically generated for each video.  

Filenames follow the pattern:

video_name.txt
